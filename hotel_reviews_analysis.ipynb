{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name_ru</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Екатеринбург, ул. Московская / ул. Волгоградск...</td>\n",
       "      <td>Московский квартал</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Жилой комплекс</td>\n",
       "      <td>Московский квартал 2.\\nШумно : летом по ночам ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Московская область, Электросталь, проспект Лен...</td>\n",
       "      <td>Продукты Ермолино</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Магазин продуктов;Продукты глубокой заморозки;...</td>\n",
       "      <td>Замечательная сеть магазинов в общем, хороший ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Краснодар, Прикубанский внутригородской округ,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Фитнес-клуб</td>\n",
       "      <td>Не знаю смутят ли кого-то данные правила, но я...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Санкт-Петербург, проспект Энгельса, 111, корп. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Пункт проката;Прокат велосипедов;Сапсёрфинг</td>\n",
       "      <td>Хорошие условия аренды. \\nДружелюбный персонал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Тверь, Волоколамский проспект, 39</td>\n",
       "      <td>Студия Beauty Brow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Салон красоты;Визажисты, стилисты;Салон бровей...</td>\n",
       "      <td>Топ мастер Ангелина топ во всех смыслах ) Немн...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address             name_ru  \\\n",
       "0  Екатеринбург, ул. Московская / ул. Волгоградск...  Московский квартал   \n",
       "1  Московская область, Электросталь, проспект Лен...   Продукты Ермолино   \n",
       "2  Краснодар, Прикубанский внутригородской округ,...             LimeFit   \n",
       "3   Санкт-Петербург, проспект Энгельса, 111, корп. 1        Snow-Express   \n",
       "4                  Тверь, Волоколамский проспект, 39  Студия Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0     3.0                                     Жилой комплекс   \n",
       "1     5.0  Магазин продуктов;Продукты глубокой заморозки;...   \n",
       "2     1.0                                        Фитнес-клуб   \n",
       "3     4.0        Пункт проката;Прокат велосипедов;Сапсёрфинг   \n",
       "4     5.0  Салон красоты;Визажисты, стилисты;Салон бровей...   \n",
       "\n",
       "                                                text  \n",
       "0  Московский квартал 2.\\nШумно : летом по ночам ...  \n",
       "1  Замечательная сеть магазинов в общем, хороший ...  \n",
       "2  Не знаю смутят ли кого-то данные правила, но я...  \n",
       "3  Хорошие условия аренды. \\nДружелюбный персонал...  \n",
       "4  Топ мастер Ангелина топ во всех смыслах ) Немн...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Читаем и выводим первые 5 строк отзывов из csv файла\n",
    "data = pd.read_csv('Data/geo-reviews-dataset-2023.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rubrics</th>\n",
       "      <th>words</th>\n",
       "      <th>reviews</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Кафе</td>\n",
       "      <td>это, что, вкусная, вкусно, все, всегда, всё, д...</td>\n",
       "      <td>58496</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ресторан</td>\n",
       "      <td>это, для, как, из, заведение, за, есть, еда, в...</td>\n",
       "      <td>56761</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Гостиница</td>\n",
       "      <td>это, за, не, на, мы, можно, место, как, из, за...</td>\n",
       "      <td>43133</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Магазин продуктов</td>\n",
       "      <td>это, что, большой, все, всегда, всё, выбор, дл...</td>\n",
       "      <td>21346</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Супермаркет</td>\n",
       "      <td>это, много, большой, все, всегда, всё, выбор, ...</td>\n",
       "      <td>19746</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rubrics                                              words  \\\n",
       "13               Кафе  это, что, вкусная, вкусно, все, всегда, всё, д...   \n",
       "22           Ресторан  это, для, как, из, заведение, за, есть, еда, в...   \n",
       "25          Гостиница  это, за, не, на, мы, можно, место, как, из, за...   \n",
       "1   Магазин продуктов  это, что, большой, все, всегда, всё, выбор, дл...   \n",
       "46        Супермаркет  это, много, большой, все, всегда, всё, выбор, ...   \n",
       "\n",
       "    reviews                                             scores  \n",
       "13    58496  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "22    56761  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "25    43133  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "1     21346  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "46    19746  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('Data/geo-reviews-dataset-2023.csv', encoding='utf-8')\n",
    "# Инициализация инструментов\n",
    "nltk.download('stopwords')\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')\n",
    "\n",
    "# Определение стоп-слов для удаления из текста\n",
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "additional_stops = {'что', 'это', 'так', 'вот', 'быть', 'как', 'в', 'к', 'на', 'руб', 'мой', 'твой', 'его', 'её', 'наш', 'ваш', 'их', 'свой', 'еще', 'очень', 'поэтому', 'однако', 'конечно'}\n",
    "stop_words.update(additional_stops)\n",
    "\n",
    "# Функция предобработки текста (приведение к нижнему регистру и удаление лишних символов)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Применение предобработки\n",
    "df['aspects'] = df['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "def find_top_words_by_rubric(vectorizer):\n",
    "\n",
    "    result = {\n",
    "        'rubrics': [],\n",
    "        'words': [],\n",
    "        'reviews': [],\n",
    "        'scores': []\n",
    "    }\n",
    "\n",
    "    #Проходимся по рубрикам\n",
    "    for rubric in df_flattened['rubrics'].unique():\n",
    "        texts = df_flattened[df_flattened['rubrics'] == rubric]['aspects']\n",
    "        total_count = texts.shape[0]\n",
    "\n",
    "        # В анализ возьмём только те рубрики, у которых есть несколько текстов\n",
    "        if total_count >= 5:\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        result['rubrics'].append(rubric)\n",
    "        result['reviews'].append(total_count)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.max(axis=0).toarray().ravel()\n",
    "\n",
    "        # Возьмём топ-20 слов для каждой рубрики\n",
    "        top_words_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_indices]\n",
    "        result['words'].append(', '.join(top_words))\n",
    "        top_scores = [str(tfidf_scores[i]) for i in top_words_indices]\n",
    "        result['scores'].append(', '.join(top_scores))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Развернём датасет по рубрикам, так как одна организация может принадлежать к списку рубрик\n",
    "df['rubrics'] = df['rubrics'].apply(lambda x: x.split(\";\"))\n",
    "df_flattened = df.explode('rubrics')\n",
    "\n",
    "# Инициализируем TF-IDF-векторизатор\n",
    "aspects_vectorizer = TfidfVectorizer(use_idf = True, max_df = 0.8, min_df = 0.1)\n",
    "\n",
    "# Создадим датафрейм с результатами анализа\n",
    "tf_idf_aspects = pd.DataFrame(find_top_words_by_rubric(aspects_vectorizer)).sort_values(by='reviews', ascending=False)\n",
    "\n",
    "tf_idf_aspects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Компуктер\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290213bc0ab9428c9ea2907361bd23db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:43:20 INFO: Downloaded file to C:\\Users\\Компуктер\\stanza_resources\\resources.json\n",
      "2025-03-23 18:43:20 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2025-03-23 18:43:21 INFO: File exists: C:\\Users\\Компуктер\\stanza_resources\\ru\\default.zip\n",
      "2025-03-23 18:43:25 INFO: Finished downloading models and saved to C:\\Users\\Компуктер\\stanza_resources\n",
      "2025-03-23 18:43:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbff8d89bcb64d708e4d43d9eded685a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:43:26 INFO: Downloaded file to C:\\Users\\Компуктер\\stanza_resources\\resources.json\n",
      "2025-03-23 18:43:26 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-03-23 18:43:26 INFO: Using device: cuda\n",
      "2025-03-23 18:43:26 INFO: Loading: tokenize\n",
      "2025-03-23 18:43:26 INFO: Loading: lemma\n",
      "2025-03-23 18:43:29 INFO: Done loading processors!\n",
      "Обучение модели по батчам: 100%|██████████| 106/106 [00:25<00:00,  4.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка данных из CSV-файла\n",
    "df = pd.read_csv('Data/geo-reviews-dataset-2023.csv', encoding='utf-8')\n",
    "df = df.loc[df['rubrics'] == 'Гостиница']  # Оставляем только гостиницы\n",
    "\n",
    "# Инициализация инструментов для обработки текста\n",
    "nltk.download('stopwords')\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')\n",
    "\n",
    "# Определение стоп-слов для удаления из текста\n",
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "additional_stops = {'что', 'это', 'так', 'вот', 'быть', 'как', 'в', 'к', 'на', 'руб', 'мой', 'твой', 'его', 'её', 'наш', 'ваш', 'их', 'свой', 'еще', 'очень', 'поэтому', 'однако', 'конечно'}\n",
    "stop_words.update(additional_stops)\n",
    "\n",
    "# Функция предобработки текста (приведение к нижнему регистру и удаление лишних символов)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яё\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Применение предобработки к отзывам\n",
    "df['clean_text'] = df['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Группировка отзывов по отелям и объединение их в один текст\n",
    "df_grouped = df.groupby('name_ru')['clean_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "df_grouped['rating'] = df.groupby('name_ru')['rating'].mean().reset_index(drop=True)  # Средний рейтинг отеля\n",
    "\n",
    "# Сохранение сгруппированных данных в CSV\n",
    "df_grouped.to_csv('Data/grouped_hotels.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Векторизация текстов отзывов с использованием TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\n",
    "X = vectorizer.fit_transform(df_grouped['clean_text'])\n",
    "y = df_grouped['rating'].values\n",
    "\n",
    "# Инициализация модели случайного леса для регрессии\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, warm_start=True)\n",
    "batch_size = 100  # Размер батча для обучения\n",
    "\n",
    "# Обучение модели по батчам\n",
    "for i in tqdm(range(0, X.shape[0], batch_size), desc=\"Обучение модели по батчам\"):\n",
    "    X_batch = X[i:i+batch_size]\n",
    "    y_batch = y[i:i+batch_size]\n",
    "    \n",
    "    if i == 0:\n",
    "        model.fit(X_batch, y_batch)  # Первоначальное обучение модели\n",
    "    else:\n",
    "        model.n_estimators += 10  # Увеличение количества деревьев в лесу\n",
    "        model.fit(X_batch, y_batch)\n",
    "\n",
    "# Сохранение обученной модели и векторайзера в файлы\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Загрузка модели и данных\n",
    "model = joblib.load(\"model.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "df_grouped = pd.read_csv(\"Data/grouped_hotels.csv\")\n",
    "\n",
    "# Функция поиска отелей\n",
    "def recommend_hotels():\n",
    "    user_input = entry.get()\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        messagebox.showwarning(\"Ошибка\", \"Введите предпочтения перед поиском!\")\n",
    "        return\n",
    "    \n",
    "    user_vector = vectorizer.transform([user_input])  # Преобразуем ввод в вектор\n",
    "    hotel_vectors = vectorizer.transform(df_grouped[\"clean_text\"])  # Векторы отелей\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, hotel_vectors).flatten()  # Считаем сходство\n",
    "    top_indices = similarities.argsort()[::-1][:5]  # Берём топ-5\n",
    "\n",
    "    result_text.config(state=tk.NORMAL)\n",
    "    result_text.delete(1.0, tk.END)\n",
    "\n",
    "    if similarities[top_indices[0]] == 0:\n",
    "        result_text.insert(tk.END, \"❌ По вашему запросу ничего не найдено.\")\n",
    "    else:\n",
    "        result_text.insert(tk.END, \"🎯 Топ 5 рекомендаций:\\n\\n\")\n",
    "        for idx, i in enumerate(top_indices, 1):\n",
    "            result_text.insert(\n",
    "                tk.END, f\"{idx}. {df_grouped['name_ru'][i]} (Рейтинг: {df_grouped['rating'][i]:.2f})\\n\"\n",
    "            )\n",
    "    \n",
    "    result_text.config(state=tk.DISABLED)\n",
    "\n",
    "# Создание GUI\n",
    "app = tk.Tk()\n",
    "app.title(\"Рекомендации отелей\")\n",
    "app.geometry(\"500x400\")\n",
    "\n",
    "# Виджеты\n",
    "tk.Label(app, text=\"Введите предпочтения:\", font=(\"Arial\", 12)).pack(pady=5)\n",
    "entry = tk.Entry(app, width=50, font=(\"Arial\", 12))\n",
    "entry.pack(pady=5)\n",
    "\n",
    "tk.Button(app, text=\"🔍 Найти отели\", font=(\"Arial\", 12), command=recommend_hotels).pack(pady=10)\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(app, width=60, height=10, font=(\"Arial\", 10), state=tk.DISABLED)\n",
    "result_text.pack(pady=5)\n",
    "\n",
    "# Запуск приложения\n",
    "app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

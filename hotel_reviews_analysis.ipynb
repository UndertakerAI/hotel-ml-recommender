{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name_ru</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥, —É–ª. –ú–æ—Å–∫–æ–≤—Å–∫–∞—è / —É–ª. –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫...</td>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª</td>\n",
       "      <td>3.0</td>\n",
       "      <td>–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å</td>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª 2.\\n–®—É–º–Ω–æ : –ª–µ—Ç–æ–º –ø–æ –Ω–æ—á–∞–º ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –≠–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–ª—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –õ–µ–Ω...</td>\n",
       "      <td>–ü—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–æ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>–ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤;–ü—Ä–æ–¥—É–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏;...</td>\n",
       "      <td>–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –æ–±—â–µ–º, —Ö–æ—Ä–æ—à–∏–π ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, –ü—Ä–∏–∫—É–±–∞–Ω—Å–∫–∏–π –≤–Ω—É—Ç—Ä–∏–≥–æ—Ä–æ–¥—Å–∫–æ–π –æ–∫—Ä—É–≥,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1.0</td>\n",
       "      <td>–§–∏—Ç–Ω–µ—Å-–∫–ª—É–±</td>\n",
       "      <td>–ù–µ –∑–Ω–∞—é —Å–º—É—Ç—è—Ç –ª–∏ –∫–æ–≥–æ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –Ω–æ —è...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ø—Ä–æ—Å–ø–µ–∫—Ç –≠–Ω–≥–µ–ª—å—Å–∞, 111, –∫–æ—Ä–ø. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4.0</td>\n",
       "      <td>–ü—É–Ω–∫—Ç –ø—Ä–æ–∫–∞—Ç–∞;–ü—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤;–°–∞–ø—Å—ë—Ä—Ñ–∏–Ω–≥</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è –∞—Ä–µ–Ω–¥—ã. \\n–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢–≤–µ—Ä—å, –í–æ–ª–æ–∫–æ–ª–∞–º—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç, 39</td>\n",
       "      <td>–°—Ç—É–¥–∏—è Beauty Brow</td>\n",
       "      <td>5.0</td>\n",
       "      <td>–°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã;–í–∏–∑–∞–∂–∏—Å—Ç—ã, —Å—Ç–∏–ª–∏—Å—Ç—ã;–°–∞–ª–æ–Ω –±—Ä–æ–≤–µ–π...</td>\n",
       "      <td>–¢–æ–ø –º–∞—Å—Ç–µ—Ä –ê–Ω–≥–µ–ª–∏–Ω–∞ —Ç–æ–ø –≤–æ –≤—Å–µ—Ö —Å–º—ã—Å–ª–∞—Ö ) –ù–µ–º–Ω...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address             name_ru  \\\n",
       "0  –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥, —É–ª. –ú–æ—Å–∫–æ–≤—Å–∫–∞—è / —É–ª. –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫...  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª   \n",
       "1  –ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –≠–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–ª—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –õ–µ–Ω...   –ü—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–æ   \n",
       "2  –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, –ü—Ä–∏–∫—É–±–∞–Ω—Å–∫–∏–π –≤–Ω—É—Ç—Ä–∏–≥–æ—Ä–æ–¥—Å–∫–æ–π –æ–∫—Ä—É–≥,...             LimeFit   \n",
       "3   –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ø—Ä–æ—Å–ø–µ–∫—Ç –≠–Ω–≥–µ–ª—å—Å–∞, 111, –∫–æ—Ä–ø. 1        Snow-Express   \n",
       "4                  –¢–≤–µ—Ä—å, –í–æ–ª–æ–∫–æ–ª–∞–º—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç, 39  –°—Ç—É–¥–∏—è Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0     3.0                                     –ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å   \n",
       "1     5.0  –ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤;–ü—Ä–æ–¥—É–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏;...   \n",
       "2     1.0                                        –§–∏—Ç–Ω–µ—Å-–∫–ª—É–±   \n",
       "3     4.0        –ü—É–Ω–∫—Ç –ø—Ä–æ–∫–∞—Ç–∞;–ü—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤;–°–∞–ø—Å—ë—Ä—Ñ–∏–Ω–≥   \n",
       "4     5.0  –°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã;–í–∏–∑–∞–∂–∏—Å—Ç—ã, —Å—Ç–∏–ª–∏—Å—Ç—ã;–°–∞–ª–æ–Ω –±—Ä–æ–≤–µ–π...   \n",
       "\n",
       "                                                text  \n",
       "0  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª 2.\\n–®—É–º–Ω–æ : –ª–µ—Ç–æ–º –ø–æ –Ω–æ—á–∞–º ...  \n",
       "1  –ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –æ–±—â–µ–º, —Ö–æ—Ä–æ—à–∏–π ...  \n",
       "2  –ù–µ –∑–Ω–∞—é —Å–º—É—Ç—è—Ç –ª–∏ –∫–æ–≥–æ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –Ω–æ —è...  \n",
       "3  –•–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è –∞—Ä–µ–Ω–¥—ã. \\n–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª...  \n",
       "4  –¢–æ–ø –º–∞—Å—Ç–µ—Ä –ê–Ω–≥–µ–ª–∏–Ω–∞ —Ç–æ–ø –≤–æ –≤—Å–µ—Ö —Å–º—ã—Å–ª–∞—Ö ) –ù–µ–º–Ω...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º –∏ –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ csv —Ñ–∞–π–ª–∞\n",
    "data = pd.read_csv('Data/geo-reviews-dataset-2023.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rubrics</th>\n",
       "      <th>words</th>\n",
       "      <th>reviews</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>–ö–∞—Ñ–µ</td>\n",
       "      <td>—ç—Ç–æ, —á—Ç–æ, –≤–∫—É—Å–Ω–∞—è, –≤–∫—É—Å–Ω–æ, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –¥...</td>\n",
       "      <td>58496</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>–†–µ—Å—Ç–æ—Ä–∞–Ω</td>\n",
       "      <td>—ç—Ç–æ, –¥–ª—è, –∫–∞–∫, –∏–∑, –∑–∞–≤–µ–¥–µ–Ω–∏–µ, –∑–∞, –µ—Å—Ç—å, –µ–¥–∞, –≤...</td>\n",
       "      <td>56761</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>–ì–æ—Å—Ç–∏–Ω–∏—Ü–∞</td>\n",
       "      <td>—ç—Ç–æ, –∑–∞, –Ω–µ, –Ω–∞, –º—ã, –º–æ–∂–Ω–æ, –º–µ—Å—Ç–æ, –∫–∞–∫, –∏–∑, –∑–∞...</td>\n",
       "      <td>43133</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤</td>\n",
       "      <td>—ç—Ç–æ, —á—Ç–æ, –±–æ–ª—å—à–æ–π, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –≤—ã–±–æ—Ä, –¥–ª...</td>\n",
       "      <td>21346</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>–°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç</td>\n",
       "      <td>—ç—Ç–æ, –º–Ω–æ–≥–æ, –±–æ–ª—å—à–æ–π, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –≤—ã–±–æ—Ä, ...</td>\n",
       "      <td>19746</td>\n",
       "      <td>1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rubrics                                              words  \\\n",
       "13               –ö–∞—Ñ–µ  —ç—Ç–æ, —á—Ç–æ, –≤–∫—É—Å–Ω–∞—è, –≤–∫—É—Å–Ω–æ, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –¥...   \n",
       "22           –†–µ—Å—Ç–æ—Ä–∞–Ω  —ç—Ç–æ, –¥–ª—è, –∫–∞–∫, –∏–∑, –∑–∞–≤–µ–¥–µ–Ω–∏–µ, –∑–∞, –µ—Å—Ç—å, –µ–¥–∞, –≤...   \n",
       "25          –ì–æ—Å—Ç–∏–Ω–∏—Ü–∞  —ç—Ç–æ, –∑–∞, –Ω–µ, –Ω–∞, –º—ã, –º–æ–∂–Ω–æ, –º–µ—Å—Ç–æ, –∫–∞–∫, –∏–∑, –∑–∞...   \n",
       "1   –ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤  —ç—Ç–æ, —á—Ç–æ, –±–æ–ª—å—à–æ–π, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –≤—ã–±–æ—Ä, –¥–ª...   \n",
       "46        –°—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç  —ç—Ç–æ, –º–Ω–æ–≥–æ, –±–æ–ª—å—à–æ–π, –≤—Å–µ, –≤—Å–µ–≥–¥–∞, –≤—Å—ë, –≤—ã–±–æ—Ä, ...   \n",
       "\n",
       "    reviews                                             scores  \n",
       "13    58496  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "22    56761  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "25    43133  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "1     21346  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  \n",
       "46    19746  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('Data/geo-reviews-dataset-2023.csv', encoding='utf-8')\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n",
    "nltk.download('stopwords')\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "additional_stops = {'—á—Ç–æ', '—ç—Ç–æ', '—Ç–∞–∫', '–≤–æ—Ç', '–±—ã—Ç—å', '–∫–∞–∫', '–≤', '–∫', '–Ω–∞', '—Ä—É–±', '–º–æ–π', '—Ç–≤–æ–π', '–µ–≥–æ', '–µ—ë', '–Ω–∞—à', '–≤–∞—à', '–∏—Ö', '—Å–≤–æ–π', '–µ—â–µ', '–æ—á–µ–Ω—å', '–ø–æ—ç—Ç–æ–º—É', '–æ–¥–Ω–∞–∫–æ', '–∫–æ–Ω–µ—á–Ω–æ'}\n",
    "stop_words.update(additional_stops)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^–∞-—è—ë\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "df['aspects'] = df['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "def find_top_words_by_rubric(vectorizer):\n",
    "\n",
    "    result = {\n",
    "        'rubrics': [],\n",
    "        'words': [],\n",
    "        'reviews': [],\n",
    "        'scores': []\n",
    "    }\n",
    "\n",
    "    #–ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ —Ä—É–±—Ä–∏–∫–∞–º\n",
    "    for rubric in df_flattened['rubrics'].unique():\n",
    "        texts = df_flattened[df_flattened['rubrics'] == rubric]['aspects']\n",
    "        total_count = texts.shape[0]\n",
    "\n",
    "        # –í –∞–Ω–∞–ª–∏–∑ –≤–æ–∑—å–º—ë–º —Ç–æ–ª—å–∫–æ —Ç–µ —Ä—É–±—Ä–∏–∫–∏, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        if total_count >= 5:\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        result['rubrics'].append(rubric)\n",
    "        result['reviews'].append(total_count)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        tfidf_scores = tfidf_matrix.max(axis=0).toarray().ravel()\n",
    "\n",
    "        # –í–æ–∑—å–º—ë–º —Ç–æ–ø-20 —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Ä—É–±—Ä–∏–∫–∏\n",
    "        top_words_indices = tfidf_scores.argsort()[-20:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_indices]\n",
    "        result['words'].append(', '.join(top_words))\n",
    "        top_scores = [str(tfidf_scores[i]) for i in top_words_indices]\n",
    "        result['scores'].append(', '.join(top_scores))\n",
    "\n",
    "    return result\n",
    "\n",
    "# –†–∞–∑–≤–µ—Ä–Ω—ë–º –¥–∞—Ç–∞—Å–µ—Ç –ø–æ —Ä—É–±—Ä–∏–∫–∞–º, —Ç–∞–∫ –∫–∞–∫ –æ–¥–Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ —Å–ø–∏—Å–∫—É —Ä—É–±—Ä–∏–∫\n",
    "df['rubrics'] = df['rubrics'].apply(lambda x: x.split(\";\"))\n",
    "df_flattened = df.explode('rubrics')\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TF-IDF-–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
    "aspects_vectorizer = TfidfVectorizer(use_idf = True, max_df = 0.8, min_df = 0.1)\n",
    "\n",
    "# –°–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞\n",
    "tf_idf_aspects = pd.DataFrame(find_top_words_by_rubric(aspects_vectorizer)).sort_values(by='reviews', ascending=False)\n",
    "\n",
    "tf_idf_aspects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\–ö–æ–º–ø—É–∫—Ç–µ—Ä\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290213bc0ab9428c9ea2907361bd23db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:43:20 INFO: Downloaded file to C:\\Users\\–ö–æ–º–ø—É–∫—Ç–µ—Ä\\stanza_resources\\resources.json\n",
      "2025-03-23 18:43:20 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2025-03-23 18:43:21 INFO: File exists: C:\\Users\\–ö–æ–º–ø—É–∫—Ç–µ—Ä\\stanza_resources\\ru\\default.zip\n",
      "2025-03-23 18:43:25 INFO: Finished downloading models and saved to C:\\Users\\–ö–æ–º–ø—É–∫—Ç–µ—Ä\\stanza_resources\n",
      "2025-03-23 18:43:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbff8d89bcb64d708e4d43d9eded685a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 18:43:26 INFO: Downloaded file to C:\\Users\\–ö–æ–º–ø—É–∫—Ç–µ—Ä\\stanza_resources\\resources.json\n",
      "2025-03-23 18:43:26 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-03-23 18:43:26 INFO: Using device: cuda\n",
      "2025-03-23 18:43:26 INFO: Loading: tokenize\n",
      "2025-03-23 18:43:26 INFO: Loading: lemma\n",
      "2025-03-23 18:43:29 INFO: Done loading processors!\n",
      "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –±–∞—Ç—á–∞–º: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106/106 [00:25<00:00,  4.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import stanza\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV-—Ñ–∞–π–ª–∞\n",
    "df = pd.read_csv('Data/geo-reviews-dataset-2023.csv', encoding='utf-8')\n",
    "df = df.loc[df['rubrics'] == '–ì–æ—Å—Ç–∏–Ω–∏—Ü–∞']  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥–æ—Å—Ç–∏–Ω–∏—Ü—ã\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "nltk.download('stopwords')\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
    "additional_stops = {'—á—Ç–æ', '—ç—Ç–æ', '—Ç–∞–∫', '–≤–æ—Ç', '–±—ã—Ç—å', '–∫–∞–∫', '–≤', '–∫', '–Ω–∞', '—Ä—É–±', '–º–æ–π', '—Ç–≤–æ–π', '–µ–≥–æ', '–µ—ë', '–Ω–∞—à', '–≤–∞—à', '–∏—Ö', '—Å–≤–æ–π', '–µ—â–µ', '–æ—á–µ–Ω—å', '–ø–æ—ç—Ç–æ–º—É', '–æ–¥–Ω–∞–∫–æ', '–∫–æ–Ω–µ—á–Ω–æ'}\n",
    "stop_words.update(additional_stops)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^–∞-—è—ë\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫ –æ—Ç–∑—ã–≤–∞–º\n",
    "df['clean_text'] = df['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –æ—Ç–µ–ª—è–º –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏—Ö –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç\n",
    "df_grouped = df.groupby('name_ru')['clean_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "df_grouped['rating'] = df.groupby('name_ru')['rating'].mean().reset_index(drop=True)  # –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –æ—Ç–µ–ª—è\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV\n",
    "df_grouped.to_csv('Data/grouped_hotels.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=list(stop_words))\n",
    "X = vectorizer.fit_transform(df_grouped['clean_text'])\n",
    "y = df_grouped['rating'].values\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, warm_start=True)\n",
    "batch_size = 100  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –±–∞—Ç—á–∞–º\n",
    "for i in tqdm(range(0, X.shape[0], batch_size), desc=\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –±–∞—Ç—á–∞–º\"):\n",
    "    X_batch = X[i:i+batch_size]\n",
    "    y_batch = y[i:i+batch_size]\n",
    "    \n",
    "    if i == 0:\n",
    "        model.fit(X_batch, y_batch)  # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    else:\n",
    "        model.n_estimators += 10  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É\n",
    "        model.fit(X_batch, y_batch)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä–∞ –≤ —Ñ–∞–π–ª—ã\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "model = joblib.load(\"model.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "df_grouped = pd.read_csv(\"Data/grouped_hotels.csv\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –æ—Ç–µ–ª–µ–π\n",
    "def recommend_hotels():\n",
    "    user_input = entry.get()\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        messagebox.showwarning(\"–û—à–∏–±–∫–∞\", \"–í–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º!\")\n",
    "        return\n",
    "    \n",
    "    user_vector = vectorizer.transform([user_input])  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–≤–æ–¥ –≤ –≤–µ–∫—Ç–æ—Ä\n",
    "    hotel_vectors = vectorizer.transform(df_grouped[\"clean_text\"])  # –í–µ–∫—Ç–æ—Ä—ã –æ—Ç–µ–ª–µ–π\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, hotel_vectors).flatten()  # –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ\n",
    "    top_indices = similarities.argsort()[::-1][:5]  # –ë–µ—Ä—ë–º —Ç–æ–ø-5\n",
    "\n",
    "    result_text.config(state=tk.NORMAL)\n",
    "    result_text.delete(1.0, tk.END)\n",
    "\n",
    "    if similarities[top_indices[0]] == 0:\n",
    "        result_text.insert(tk.END, \"‚ùå –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
    "    else:\n",
    "        result_text.insert(tk.END, \"üéØ –¢–æ–ø 5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:\\n\\n\")\n",
    "        for idx, i in enumerate(top_indices, 1):\n",
    "            result_text.insert(\n",
    "                tk.END, f\"{idx}. {df_grouped['name_ru'][i]} (–†–µ–π—Ç–∏–Ω–≥: {df_grouped['rating'][i]:.2f})\\n\"\n",
    "            )\n",
    "    \n",
    "    result_text.config(state=tk.DISABLED)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ GUI\n",
    "app = tk.Tk()\n",
    "app.title(\"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç–µ–ª–µ–π\")\n",
    "app.geometry(\"500x400\")\n",
    "\n",
    "# –í–∏–¥–∂–µ—Ç—ã\n",
    "tk.Label(app, text=\"–í–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è:\", font=(\"Arial\", 12)).pack(pady=5)\n",
    "entry = tk.Entry(app, width=50, font=(\"Arial\", 12))\n",
    "entry.pack(pady=5)\n",
    "\n",
    "tk.Button(app, text=\"üîç –ù–∞–π—Ç–∏ –æ—Ç–µ–ª–∏\", font=(\"Arial\", 12), command=recommend_hotels).pack(pady=10)\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(app, width=60, height=10, font=(\"Arial\", 10), state=tk.DISABLED)\n",
    "result_text.pack(pady=5)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è\n",
    "app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
